{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BigQuery Python client library provides a magic command that allows you to run queries with minimal code. Type the following Python code into the next cell to import the BigQuery Python client library and initialize a client. The BigQuery client is used to send and receive messages from the BigQuery API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# \n",
    "#!{sys.executable} -m pip install --upgrade google-cloud-bigquery matplotlib numpy scipy\n",
    "from google.cloud import bigquery\n",
    "client = bigquery.Client()\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "\n",
    "import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the Workflow ID you want to plot along with its subworkflows, main workflow first. Also set the cromwell execution bucket the workflow directory is expected to exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control 2020-03-20\n",
    "#workflow_ids= [\"\\\"8bcb5d62-434b-4982-9969-8d50d99a4a98\\\"\", #Control | main | small dataset | 47\n",
    "#               \"\\\"c3df9d5d-5ec4-4299-8d4a-ccf6c364340f\\\"\"] # Control | sub | small dataset | 47 | ScatterAt32_18\n",
    "\n",
    "# Control 05/09/2020\n",
    "#workflow_ids= [\"\\\"a0a92771-c925-49b4-887e-877deeacc742\\\"\", # Control | main | typical dataset | 47\n",
    "#               \"\\\"9837e566-0f37-444b-be3e-a135616f4c6e\\\"\"] # Control | sub | typical dataset | 47 | ScatterAt32_18\n",
    "\n",
    "# Spark 2020/05/08\n",
    "#workflow_ids= [\"\\\"93b8224b-e7f1-4226-adfa-01f3da15373d\\\"\", # Spark | main | typical dataset | 45\n",
    "#               \"\\\"2947c1f7-1a84-4724-abb8-c7c25c047dd5\\\"\", # Spark | sub | typical dataset | 45 PBCCSOnlySingleFlowcell.ShardUBAM\n",
    "#               \"\\\"6e0a7851-8618-4c40-a04d-a6c0ce125be6\\\"\"] # Spark | sub | typical dataset | 45 ScatterAt38_18\n",
    "\n",
    "# Bri 05/29/2020\n",
    "workflow_ids= [\"\\\"8d61ce56-9d67-4d25-9c54-fe9d09a7830e\\\"\", # Bri | main | typical dataset | 47\n",
    "               \"\\\"237f38b5-0e01-442b-b876-a6b07a03ea47\\\"\"] # Bri | sub | typical dataset | 47\n",
    "#-------------------------\n",
    "PARENT_WORKFLOW_ID = workflow_ids[0].strip('\"')\n",
    "formated_workflow_ids = ','.join(workflow_ids)\n",
    "\n",
    "CROMWELL_EXEC_BUCKET = \"broad-methods-cromwell-exec-bucket-v47\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Client.query() method to run a query. In the next cell, enter the following code to run a query to retrieve the annual count of plural births by plurality (2 for twins, 3 for triplets, and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "    \n",
    "SELECT\n",
    "  *,\n",
    "  TIMESTAMP_DIFF(meta_end_time, meta_start_time, SECOND) meta_duration_sec\n",
    "FROM\n",
    "  #broad-dsde-methods.cromwell_monitoring.monitor_everything_back_7days\n",
    "  broad-dsde-methods.cromwell_monitoring.monitor_everything_between_dates\n",
    "WHERE\n",
    "  runtime_workflow_id IN ({formated_workflow_ids})\n",
    "    \n",
    "\"\"\"\n",
    "df_monitoring = client.query(sql).to_dataframe()\n",
    "## Show head of table \n",
    "df_monitoring.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TaskNames = df_monitoring.meta_task_call_name.unique()\n",
    "print(\"The tasks in this workflow are:\" + \"\\n\" + \"--------\")\n",
    "print(*TaskNames, sep =\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell plots the monitoring data from the datafram per task per shard (10 longest running shards). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "### Change the plot size here \n",
    "plt.rcParams[\"figure.figsize\"] = (15, 20)\n",
    "\n",
    "with PdfPages(PARENT_WORKFLOW_ID + '_resource_monitoring.pdf') as pdf:\n",
    "\n",
    "    for task_name in TaskNames:\n",
    "        \n",
    "        # Gets the all shards for a given task name\n",
    "        shards = df_monitoring.meta_shard.loc[(df_monitoring['meta_task_call_name'] == task_name)]\n",
    "        shards = shards.sort_values().unique()\n",
    "        \n",
    "        # If shard counts is greater than 10 then gets 10 longest running shards for a given task name\n",
    "        max_shards=10\n",
    "        if len(shards) > max_shards:\n",
    "            #create and sort meta table by duration\n",
    "            df_monitoring_task = df_monitoring.loc[(df_monitoring['meta_task_call_name'] == task_name)]\n",
    "            df_monitoring_task_sorted_duration = df_monitoring_task.sort_values(by='meta_duration_sec', ascending=False)\n",
    "            #replace all shards in varaible shards with the first 50 of the sorted duration table\n",
    "            shards = df_monitoring_task_sorted_duration.meta_shard.head(max_shards)\n",
    "        \n",
    "        for shard in shards:\n",
    "\n",
    "            df_monitoring_task_shard = df_monitoring.loc[(df_monitoring['meta_task_call_name'] == task_name) & (df_monitoring['meta_shard'] == shard)]\n",
    "            df_monitoring_task_shard = df_monitoring_task_shard.sort_values(by='metrics_timestamp')\n",
    "            task_shard_meta_duration = df_monitoring_task_shard.meta_duration_sec.iloc[0]\n",
    "            task_shard_duration = datetime.timedelta.total_seconds(max(df_monitoring_task_shard['metrics_timestamp'])-min(df_monitoring_task_shard['metrics_timestamp']))\n",
    "            \n",
    "            # create an array for list coloumns\n",
    "            cpu_used_percent_array = [np.asarray(x).max() for x in df_monitoring_task_shard.metrics_cpu_used_percent]\n",
    "            disk_used_gb_array = [np.asarray(x).max() for x in df_monitoring_task_shard.metrics_disk_used_gb]\n",
    "            disk_read_iops_array = [np.asarray(x).max() for x in df_monitoring_task_shard.metrics_disk_read_iops]\n",
    "            disk_write_iops_array = [np.asarray(x).max() for x in df_monitoring_task_shard.metrics_disk_write_iops]\n",
    "\n",
    "            runtime_list= df_monitoring_task_shard.iloc[0].at['meta_inputs']\n",
    "            runtime_dic={}\n",
    "            for i, element in enumerate(runtime_list):\n",
    "                if re.search(\"default_attr\", element[\"key\"]) or re.search(\"runtime_attr_override\", element[\"key\"]):\n",
    "                    continue\n",
    "                else:\n",
    "                    k = element[\"key\"].replace(\"[\", \"\").replace(\"]\", \"\").replace(\"runtime_attr\", \"\").replace(\"\\\"\", \"\", 2)\n",
    "                    v = element[\"value\"]\n",
    "                    runtime_dic[k]=v\n",
    "\n",
    "            plt.subplot(5, 1, 1)\n",
    "            plt.title(\"Task Name: \" + task_name + \" Shard: \" + str(shard) + \" Duration: \" +  str(task_shard_meta_duration), fontsize=20)\n",
    "            plt.plot(df_monitoring_task_shard.metrics_timestamp.astype('O'), cpu_used_percent_array, label='CPU Used')\n",
    "            plt.plot([], [], ' ', label='Obtained CPU Cores: {}' .format(df_monitoring_task_shard.iloc[0].at['meta_cpu']))\n",
    "            plt.plot([], [], ' ', label='Requested CPU Cores: {}' .format(runtime_dic[\"cpu_cores\"]))\n",
    "            plt.legend(loc='upper center', bbox_to_anchor=(1.20, 0.8), shadow=True, ncol=1)\n",
    "            plt.ylabel('CPU Percentage Used')\n",
    "            plt.xlabel(\"Date Time\")\n",
    "            plt2 = plt.twiny()\n",
    "            plt2.set_xlim(0, task_shard_duration)\n",
    "            plt2.set_xlabel(\"Duration Time\")\n",
    "            plt.grid(True)\n",
    "\n",
    "            plt.subplot(5, 1, 2)\n",
    "            plt.plot(df_monitoring_task_shard.metrics_timestamp.astype('O'), df_monitoring_task_shard.metrics_mem_used_gb, label='Memory Used')\n",
    "            plt.axhline(y=df_monitoring_task_shard.iloc[0].at['meta_mem_total_gb'], color='r', label='Max Memory GB: %.2f' %(df_monitoring_task_shard.iloc[0].at['meta_mem_total_gb']))\n",
    "            plt.plot([], [], ' ', label='Requested Memory GB: {}' .format(runtime_dic[\"mem_gb\"]))\n",
    "            plt.legend(loc='upper center', bbox_to_anchor=(1.20, 0.8), shadow=True, ncol=1)\n",
    "            plt.ylabel('Memory Used in GB')\n",
    "            plt.xlabel(\"Date Time\")\n",
    "            plt2 = plt.twiny()\n",
    "            plt2.set_xlim(0, task_shard_duration)\n",
    "            plt2.set_xlabel(\"Duration Time\")\n",
    "            plt.grid(True)\n",
    "\n",
    "            plt.subplot(5, 1, 3)\n",
    "            plt.plot(df_monitoring_task_shard.metrics_timestamp.astype('O'), disk_used_gb_array, label='Disk Used')\n",
    "            plt.axhline(y=max(df_monitoring_task_shard.iloc[0].at['meta_disk_total_gb']), color='r', label='Max Disksize GB: %.2f' %(max(df_monitoring_task_shard.iloc[0].at['meta_disk_total_gb'])))\n",
    "            plt.plot([], [], ' ', label='Requested Disksize GB: {}' .format(runtime_dic[\"disk_gb\"]))\n",
    "            plt.legend(loc='upper center', bbox_to_anchor=(1.20, 0.8), shadow=True, ncol=1)\n",
    "            plt.ylabel('Diskspace Used in GB')\n",
    "            plt.xlabel(\"Date Time\")\n",
    "            plt2 = plt.twiny()\n",
    "            plt2.set_xlim(0, task_shard_duration)\n",
    "            plt2.set_xlabel(\"Duration Time\")\n",
    "            plt.grid(True)\n",
    "\n",
    "            plt.subplot(5, 1, 4)\n",
    "            plt.plot(df_monitoring_task_shard.metrics_timestamp.astype('O'), disk_read_iops_array)\n",
    "            plt.ylabel('Disk Read IOps')\n",
    "            plt.xlabel(\"Date Time\")\n",
    "            plt2 = plt.twiny()\n",
    "            plt2.set_xlim(0, task_shard_duration)\n",
    "            plt2.set_xlabel(\"Duration Time\")\n",
    "            plt.grid(True)\n",
    "\n",
    "            plt.subplot(5, 1, 5)\n",
    "            plt.plot(df_monitoring_task_shard.metrics_timestamp.astype('O'), disk_write_iops_array)\n",
    "            plt.ylabel('Disk Write_IOps')\n",
    "            plt.xlabel(\"Date Time\")\n",
    "            plt2 = plt.twiny()\n",
    "            plt2.set_xlim(0, task_shard_duration)\n",
    "            plt2.set_xlabel(\"Duration Time\")\n",
    "            plt.grid(True)\n",
    "\n",
    "            pdf.savefig(bbox_inches='tight', pad_inches=0.5)\n",
    "            plt.subplots_adjust(hspace = 0.5)\n",
    "            plt.show()\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell saves the plot pdf into the working directory of the workflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requires that user (or Terra user proxy) has edit access to destination bucket\n",
    "WORKFLOW_NAME=df_meta.iloc[0].at['workflow_name'] \n",
    "!gsutil cp ./{PARENT_WORKFLOW_ID}_resource_monitoring.pdf gs://{CROMWELL_EXEC_BUCKET}/{WORKFLOW_NAME}/{WORKFLOWID}/{WORKFLOWID}_resource_monitoring.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "58px",
    "width": "276px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
